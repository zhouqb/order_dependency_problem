{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ed6fca-4518-4a8c-92e0-872e8f4a5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ded370-75c8-4d7f-99e0-f2e2c211a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import order_dependency_problem\n",
    "from order_dependency_problem.dataset import ArcDataset\n",
    "from order_dependency_problem.quesiton_answering import (\n",
    "    answer_question, answer_multiple_questions\n",
    ")\n",
    "from order_dependency_problem.evaluation import (\n",
    "    calculate_answer_prevalence, calculate_accuracy, calculate_answer_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270af09c-df3c-4328-93eb-a0f4cdf2658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "ARC_DATASET_PATH = Path(order_dependency_problem.__file__).parent.parent.parent / \"data/arc/ARC-Challenge-Test.jsonl\"\n",
    "\n",
    "output_json = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b8343-2dec-41cd-9944-52ac40f87d0c",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "100 sample questions from ARC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4680c2d1-281a-4a83-96fc-8033c2c253e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_dataset = ArcDataset.load_from_file(ARC_DATASET_PATH, num_samples=100, seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec9e3f1-8f7a-456e-985d-03d0dbd08a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = {}\n",
    "for question in arc_dataset.questions:\n",
    "    for choice in question.choices:\n",
    "        if choice.is_correct_answer:\n",
    "            correct_answers[question.id] = choice.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59d120-2cc6-43e1-88d8-635eaf619a99",
   "metadata": {},
   "source": [
    "# Explore the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffcb1d69-3ccc-4f61-82ce-d07bf0845dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>ground_truth_count</th>\n",
       "      <th>answer_prevalence</th>\n",
       "      <th>answer_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>A</td>\n",
       "      <td>19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>B</td>\n",
       "      <td>30</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>C</td>\n",
       "      <td>24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>D</td>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>A</td>\n",
       "      <td>19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>B</td>\n",
       "      <td>30</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>C</td>\n",
       "      <td>24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>D</td>\n",
       "      <td>27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model label  ground_truth_count  answer_prevalence  answer_recall\n",
       "7  gpt-3.5-turbo     A                  19               0.18       0.894737\n",
       "4  gpt-3.5-turbo     B                  30               0.32       0.833333\n",
       "6  gpt-3.5-turbo     C                  24               0.25       0.791667\n",
       "5  gpt-3.5-turbo     D                  27               0.25       0.703704\n",
       "3    gpt-4o-mini     A                  19               0.23       1.000000\n",
       "0    gpt-4o-mini     B                  30               0.31       0.900000\n",
       "2    gpt-4o-mini     C                  24               0.24       0.875000\n",
       "1    gpt-4o-mini     D                  27               0.22       0.777778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.069446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.079066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy  recall_std\n",
       "1  gpt-3.5-turbo      0.80    0.069446\n",
       "0    gpt-4o-mini      0.88    0.079066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_json[\"original dataset\"] = {\n",
    "    \"questions\": [question.dict() for question in arc_dataset.questions],\n",
    "    \"results\": {},\n",
    "}\n",
    "\n",
    "ground_truth_counts = collections.defaultdict(lambda: 0)\n",
    "for question in arc_dataset.questions:\n",
    "    for choice in question.choices:\n",
    "        if choice.is_correct_answer:\n",
    "            ground_truth_counts[choice.label] += 1\n",
    "\n",
    "option_level_data = []\n",
    "overall_metrics = []\n",
    "for model_name in MODEL_NAMES:\n",
    "    output_json[\"original dataset\"][\"results\"][model_name] = {}\n",
    "    responses = await answer_multiple_questions(\n",
    "        arc_dataset.questions,\n",
    "        model_name=model_name,\n",
    "        batch_size=10,\n",
    "        label_removed=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    answers = [response.content for response in responses]\n",
    "    output_json[\"original dataset\"][\"results\"][model_name][\"answers\"] = answers\n",
    "\n",
    "    # Option level metrics\n",
    "    answer_prevalences = calculate_answer_prevalence(\n",
    "        arc_dataset.questions, answers, label_removed=False\n",
    "    )\n",
    "    \n",
    "    answer_recalls = calculate_answer_recall(\n",
    "        arc_dataset.questions, answers, label_removed=False\n",
    "    )\n",
    "    option_level_data.extend(\n",
    "        [(model_name, key, ground_truth_count, answer_prevalences.get(key, 0), answer_recalls.get(key))\n",
    "         for key, ground_truth_count in ground_truth_counts.items()]\n",
    "    )\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = calculate_accuracy(\n",
    "        questions=arc_dataset.questions,\n",
    "        answers=answers,\n",
    "    )\n",
    "    recall_std = np.array(list(answer_recalls.values())).std()\n",
    "    overall_metrics.append((model_name, accuracy, recall_std))\n",
    "    output_json[\"original dataset\"][\"results\"][model_name][\"accuracy\"] = accuracy\n",
    "    output_json[\"original dataset\"][\"results\"][model_name][\"recall_std\"] = recall_std\n",
    "\n",
    "\n",
    "option_level_metrics_df = pd.DataFrame(\n",
    "    option_level_data,\n",
    "    columns=[\"model\", \"label\", \"ground_truth_count\", \"answer_prevalence\", \"answer_recall\"]\n",
    ").sort_values(by=[\"model\", \"label\"])\n",
    "                           \n",
    "overall_metrics_df = pd.DataFrame(\n",
    "    overall_metrics,\n",
    "    columns=[\"model\", \"accuracy\", \"recall_std\"],\n",
    ").sort_values(by=\"model\")\n",
    "\n",
    "display(option_level_metrics_df)\n",
    "display(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be3a8a-a925-4677-a190-9a59067325c5",
   "metadata": {},
   "source": [
    "Exploratory analysis shows:\n",
    "* Answer prevalence has a strong correlation with ground truth prevalence. We will not use it in the following analysis.\n",
    "* `gpt-4o-mini` has higher accuracy and higher recall standard deviation than `gpt-3.5-turbo`. which implies `gpt-4o-mini` may have a more severe order dependency problem.\n",
    "* From the answer recalls, Both `gpt-3.5-turbo` and `gpt-4o-mini` appear to prefer \"Option A\" and dislike \"Option D\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb06d38-5c83-4124-b0ef-35e539015344",
   "metadata": {},
   "source": [
    "# Answer-moving attack\n",
    "We will experiment moving all ground truths to a specific position (A/B/C/D) and check how it impacts answer accuracy. High accuracy fluctuation indicates high ODP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0df3df-a8ac-4575-b770-625e40d77186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.36s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.34s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.34s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>original</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  original     A     B     C     D\n",
       "0    gpt-4o-mini      0.88  0.95  0.92  0.88  0.84\n",
       "1  gpt-3.5-turbo      0.80  0.86  0.82  0.81  0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\": MODEL_NAMES,\n",
    "    \"original\": [\n",
    "        overall_metrics_df[overall_metrics_df.model == model_name].iloc[0][\"accuracy\"]\n",
    "        for model_name in MODEL_NAMES\n",
    "    ],\n",
    "}\n",
    "\n",
    "output_json[\"answer-moving attack\"] = {}\n",
    "\n",
    "for gt_label in \"ABCD\":\n",
    "    output_json[\"answer-moving attack\"][gt_label] = {}\n",
    "    questions = arc_dataset.move_ground_truth_to_option(gt_label)\n",
    "    output_json[\"answer-moving attack\"][gt_label][\"questions\"] = [question.dict() for question in questions]\n",
    "    output_json[\"answer-moving attack\"][gt_label][\"results\"] = {}\n",
    "    data[gt_label] = []\n",
    "    for model_name in MODEL_NAMES:\n",
    "        output_json[\"answer-moving attack\"][gt_label][\"results\"][model_name] = {}\n",
    "        responses = await answer_multiple_questions(\n",
    "            questions=questions,\n",
    "            model_name=model_name,\n",
    "            batch_size=10,\n",
    "            label_removed=False,\n",
    "            verbose=True,\n",
    "        )\n",
    "        answers = [response.content for response in responses]\n",
    "        accuracy = calculate_accuracy(questions, answers, label_removed=False)\n",
    "        output_json[\"answer-moving attack\"][gt_label][\"results\"][model_name][\"answers\"] = answers\n",
    "        output_json[\"answer-moving attack\"][gt_label][\"results\"][model_name][\"accuracy\"] = accuracy\n",
    "        data[gt_label].append(accuracy)\n",
    "\n",
    "answer_moving_attack_df = pd.DataFrame(data)\n",
    "display(answer_moving_attack_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554ad62-bfff-4a47-8f80-0396c6e8f119",
   "metadata": {},
   "source": [
    "The accuracy fluctuations in answer-moving attacks show:\n",
    "* Both models show similar order of accuracy fluctuations\n",
    "* Both models show moving ground truths to A (D) gets the most accuracy increase (decrease). It's consistent with the answer recall distributions shown in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766fa22-6c85-4e36-b583-962c2b284cf8",
   "metadata": {},
   "source": [
    "# Shuffle option contents\n",
    "Now let's experiment shuffling option contents, but maintain the option ID orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6c59f8-7a7a-4d9e-a37c-35803042b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.10s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.047416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.050111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy  recall_std\n",
       "1  gpt-3.5-turbo      0.82    0.047416\n",
       "0    gpt-4o-mini      0.91    0.050111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>A</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>B</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>C</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>D</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>A</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>B</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>C</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>D</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model label    recall\n",
       "4  gpt-3.5-turbo     A  0.828571\n",
       "6  gpt-3.5-turbo     B  0.736842\n",
       "7  gpt-3.5-turbo     C  0.833333\n",
       "5  gpt-3.5-turbo     D  0.863636\n",
       "0    gpt-4o-mini     A  0.971429\n",
       "2    gpt-4o-mini     B  0.842105\n",
       "1    gpt-4o-mini     C  0.916667\n",
       "3    gpt-4o-mini     D  0.863636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall_std\": [],\n",
    "}\n",
    "\n",
    "recalls = []\n",
    "\n",
    "questions = arc_dataset.generate_samples(\n",
    "    shuffle_contents=True,\n",
    "    shuffle_labels=False,\n",
    "    seed=1000,\n",
    ")\n",
    "\n",
    "output_json[\"shuffle option contents\"] = {\n",
    "    \"questions\": [question.dict() for question in questions],\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    data[\"model\"].append(model_name)\n",
    "    output_json[\"shuffle option contents\"][\"results\"][model_name] = {}\n",
    "    responses = await answer_multiple_questions(\n",
    "        questions=questions,\n",
    "        model_name=model_name,\n",
    "        batch_size=10,\n",
    "        label_removed=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    answers = [response.content for response in responses]\n",
    "    output_json[\"shuffle option contents\"][\"results\"][model_name][\"answers\"] = answers\n",
    "    \n",
    "    accuracy = calculate_accuracy(questions, answers, label_removed=False)\n",
    "    output_json[\"shuffle option contents\"][\"results\"][model_name][\"accuracy\"] = accuracy\n",
    "    data[\"accuracy\"].append(accuracy)\n",
    "    \n",
    "    answer_recalls = calculate_answer_recall(\n",
    "        questions, answers, label_removed=False\n",
    "    )\n",
    "    output_json[\"shuffle option contents\"][\"results\"][model_name][\"recalls\"] = answer_recalls\n",
    "    recalls.extend([\n",
    "        (model_name, label, recall)\n",
    "        for label, recall in answer_recalls.items()\n",
    "    ])\n",
    "    recall_std = np.array(list(answer_recalls.values())).std()\n",
    "    output_json[\"shuffle option contents\"][\"results\"][model_name][\"recall_std\"] = recall_std\n",
    "    data[\"recall_std\"].append(recall_std)\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by=\"model\")\n",
    "display(df)\n",
    "\n",
    "recall_df = pd.DataFrame(recalls, columns=[\"model\", \"label\", \"recall\"]).sort_values(by=[\"model\", \"label\"])\n",
    "display(recall_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc27afc-f56f-4aac-8a68-dd25d417f748",
   "metadata": {},
   "source": [
    "Both `gpt-4o-mini` and `gpt-3.5-turbo` have higher accuracy and recall_std. If this is repeatable on a larger dataset, it will mean that shuffling option content could help mitigate order dependency problem and hence improve answer accuracy.\n",
    "One thing to notice is that \"Option D\" has the highest recall for `gpt-3.5-turbo`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498db5c-37dc-461b-b986-f3bd9c3c8f2e",
   "metadata": {},
   "source": [
    "# Shuffle option IDs\n",
    "Now let's experiment shuffling option IDs, but maintain the option content orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6642ba02-8725-4d32-a861-450a8c7dc352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.44s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.047958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.042005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy  recall_std\n",
       "1  gpt-3.5-turbo      0.77    0.047958\n",
       "0    gpt-4o-mini      0.87    0.042005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>A</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>B</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>C</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>D</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>A</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>C</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>D</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model label    recall\n",
       "4  gpt-3.5-turbo     A  0.804878\n",
       "5  gpt-3.5-turbo     B  0.791667\n",
       "6  gpt-3.5-turbo     C  0.681818\n",
       "7  gpt-3.5-turbo     D  0.769231\n",
       "0    gpt-4o-mini     A  0.926829\n",
       "1    gpt-4o-mini     B  0.833333\n",
       "2    gpt-4o-mini     C  0.818182\n",
       "3    gpt-4o-mini     D  0.846154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall_std\": [],\n",
    "}\n",
    "\n",
    "recalls = []\n",
    "\n",
    "questions = arc_dataset.generate_samples(\n",
    "    shuffle_contents=False,\n",
    "    shuffle_labels=True,\n",
    "    seed=1000,\n",
    ")\n",
    "\n",
    "output_json[\"shuffle option ids\"] = {\n",
    "    \"questions\": [question.dict() for question in questions],\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    data[\"model\"].append(model_name)\n",
    "    output_json[\"shuffle option ids\"][\"results\"][model_name] = {}\n",
    "    responses = await answer_multiple_questions(\n",
    "        questions=questions,\n",
    "        model_name=model_name,\n",
    "        batch_size=10,\n",
    "        label_removed=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    answers = [response.content for response in responses]\n",
    "    output_json[\"shuffle option ids\"][\"results\"][model_name][\"answers\"] = answers\n",
    "    \n",
    "    accuracy = calculate_accuracy(questions, answers, label_removed=False)\n",
    "    output_json[\"shuffle option ids\"][\"results\"][model_name][\"accuracy\"] = accuracy\n",
    "    data[\"accuracy\"].append(accuracy)\n",
    "    \n",
    "    answer_recalls = calculate_answer_recall(\n",
    "        questions, answers, label_removed=False\n",
    "    )\n",
    "    output_json[\"shuffle option ids\"][\"results\"][model_name][\"recalls\"] = answer_recalls\n",
    "    recalls.extend([\n",
    "        (model_name, label, recall)\n",
    "        for label, recall in answer_recalls.items()\n",
    "    ])\n",
    "    recall_std = np.array(list(answer_recalls.values())).std()\n",
    "    output_json[\"shuffle option ids\"][\"results\"][model_name][\"recall_std\"] = recall_std\n",
    "    data[\"recall_std\"].append(recall_std)\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by=\"model\")\n",
    "display(df)\n",
    "\n",
    "recall_df = pd.DataFrame(recalls, columns=[\"model\", \"label\", \"recall\"]).sort_values(by=[\"model\", \"label\"])\n",
    "display(recall_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcceff-c099-4f37-85a3-85ee4d031504",
   "metadata": {},
   "source": [
    "* As expected, both models have lower accuracy when option IDs are shuffled (unnatural option ID order)\n",
    "* both models have lower recall_std. If this is repeated in a larger dataset, it may indicate that position bias is also important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9458d4f-e8b0-4d21-9638-20a3c8e5e1a4",
   "metadata": {},
   "source": [
    "# Remove option IDs\n",
    "Now let's experiment removing option IDs, but maintain the option orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8c7242-f3c0-4a88-b2cf-14f0eead60d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.45s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.049251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.050518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy  recall_std\n",
       "1  gpt-3.5-turbo      0.83    0.049251\n",
       "0    gpt-4o-mini      0.91    0.050518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>position_idx</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>2</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  position_idx    recall\n",
       "7  gpt-3.5-turbo             0  0.894737\n",
       "4  gpt-3.5-turbo             1  0.866667\n",
       "6  gpt-3.5-turbo             2  0.791667\n",
       "5  gpt-3.5-turbo             3  0.777778\n",
       "3    gpt-4o-mini             0  1.000000\n",
       "0    gpt-4o-mini             1  0.866667\n",
       "2    gpt-4o-mini             2  0.916667\n",
       "1    gpt-4o-mini             3  0.888889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall_std\": [],\n",
    "}\n",
    "\n",
    "recalls = []\n",
    "\n",
    "questions = copy.deepcopy(arc_dataset.questions)\n",
    "for question in questions:\n",
    "    for choice in question.choices:\n",
    "        choice.label = None\n",
    "\n",
    "output_json[\"remove option ids\"] = {\n",
    "    \"questions\": [question.dict() for question in questions],\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    data[\"model\"].append(model_name)\n",
    "    output_json[\"remove option ids\"][\"results\"][model_name] = {}\n",
    "    responses = await answer_multiple_questions(\n",
    "        questions=questions,\n",
    "        model_name=model_name,\n",
    "        batch_size=10,\n",
    "        label_removed=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    answers = [response.content for response in responses]\n",
    "    output_json[\"remove option ids\"][\"results\"][model_name][\"answers\"] = answers\n",
    "    \n",
    "    accuracy = calculate_accuracy(questions, answers, label_removed=True)\n",
    "    output_json[\"remove option ids\"][\"results\"][model_name][\"accuracy\"] = accuracy\n",
    "    data[\"accuracy\"].append(accuracy)\n",
    "    \n",
    "    answer_recalls = calculate_answer_recall(\n",
    "        questions, answers, label_removed=True\n",
    "    )\n",
    "    output_json[\"remove option ids\"][\"results\"][model_name][\"recalls\"] = answer_recalls\n",
    "    recalls.extend([\n",
    "        (model_name, label, recall)\n",
    "        for label, recall in answer_recalls.items()\n",
    "    ])\n",
    "    recall_std = np.array(list(answer_recalls.values())).std()\n",
    "    output_json[\"remove option ids\"][\"results\"][model_name][\"recall_std\"] = recall_std\n",
    "    data[\"recall_std\"].append(recall_std)\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by=\"model\")\n",
    "display(df)\n",
    "\n",
    "recall_df = pd.DataFrame(recalls, columns=[\"model\", \"position_idx\", \"recall\"]).sort_values(by=[\"model\", \"position_idx\"])\n",
    "display(recall_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81fef6-341a-4e51-b571-26bdc218fdbe",
   "metadata": {},
   "source": [
    "* As expected, both models have lower accuracy when option IDs are shuffled (unnatural option ID order)\n",
    "* both models have lower recall_std, but not as much a drop as reported in the paper. If this is repeated in a larger dataset, it may indicate that token bias plays a weaker role in ODP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949202f-dabe-41e7-a377-0771d20fe9bc",
   "metadata": {},
   "source": [
    "# Export result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f8e3dc8-b543-4f66-ac5e-1b8afe1c1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"report.json\", \"w\") as f:\n",
    "    json.dump(output_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f93357-ef3b-46e6-bd5e-6cc278263c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
